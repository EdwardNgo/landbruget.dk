name: DMI Climate Data Pipeline

on:
  workflow_dispatch:
    inputs:
      parameter:
        description: 'Climate parameter ID to fetch'
        required: true
        type: string
        default: 'temperature'
      days:
        description: 'Number of days of data to fetch'
        type: number
        required: false
        default: 30
      test_mode:
        description: 'Run in test mode with limited data'
        type: boolean
        default: false
      log_level:
        description: 'Logging level'
        type: choice
        required: true
        options:
          - WARNING
          - INFO
          - DEBUG
          - ERROR
        default: 'WARNING'

jobs:
  run-pipeline:
    runs-on: ubuntu-latest

    # Add permissions for GCS access
    permissions:
      contents: 'read'
      id-token: 'write'

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install uv
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH

    - name: Install dependencies
      run: |
        cd backend/pipelines/dmi_pipeline
        uv pip install --system -e .

    - id: 'auth'
      name: 'Authenticate to Google Cloud'
      uses: 'google-github-actions/auth@v2'
      with:
        credentials_json: '${{ secrets.GCP_SA_KEY }}'

    - name: 'Set up Cloud SDK'
      uses: 'google-github-actions/setup-gcloud@v2'

    - name: Set up environment variables
      env:
        GCS_BUCKET: landbrugsdata-raw-data
        DMI_GOV_CLOUD_API_KEY: ${{ secrets.DMI_GOV_CLOUD_API_KEY }}
        GOOGLE_CLOUD_PROJECT: ${{ secrets.GOOGLE_CLOUD_PROJECT }}
      run: |
        cd backend/pipelines/dmi_pipeline
        echo "GCS_BUCKET=$GCS_BUCKET" >> .env
        echo "DMI_GOV_CLOUD_API_KEY=$DMI_GOV_CLOUD_API_KEY" >> .env
        echo "GOOGLE_CLOUD_PROJECT=$GOOGLE_CLOUD_PROJECT" >> .env

        # Create data directories
        mkdir -p /tmp/data/bronze/dmi
        mkdir -p /tmp/data/silver/dmi

    - name: Run Pipeline
      working-directory: backend/pipelines/dmi_pipeline
      run: |
        # Determine log level: use input if available (workflow_dispatch), otherwise default to WARNING (schedule)
        LOG_LEVEL=${{ inputs.log_level || 'WARNING' }}

        # Build base command with common arguments
        CMD="python main.py --parameter ${{ inputs.parameter }} --log-level $LOG_LEVEL"

        # Add days argument
        CMD="$CMD --days ${{ inputs.days || 30 }}"

        # Add progress flag if enabled
        if [ "${{ inputs.show_progress }}" = "true" ]; then
          CMD="$CMD --progress"
        fi

        # Add test mode flag if enabled
        if [ "${{ inputs.test_mode }}" = "true" ]; then
          CMD="$CMD --test"
        fi

        echo "Running command: $CMD"
        $CMD

    - name: Handle pipeline failure
      if: failure()
      run: |
        echo "Pipeline failed. Please check the logs above for details."
        exit 1

    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      if: always()  # Upload even if the pipeline fails
      with:
        name: dmi-pipeline-output
        path: |
          backend/pipelines/dmi_pipeline/data/bronze/dmi
          backend/pipelines/dmi_pipeline/data/silver/dmi
        retention-days: 7